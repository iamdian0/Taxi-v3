{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "P1_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2550038c973a413694efb8a4cd109533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f64013e08fe9496abb85069cd6d8017a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0073afd7a7e6461f9ac81b57a9473a56",
              "IPY_MODEL_fe39a49fbb4a4dfaa459ebdf1b6ebed2"
            ]
          }
        },
        "f64013e08fe9496abb85069cd6d8017a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0073afd7a7e6461f9ac81b57a9473a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1587c0d296d441ee8eb5d293326f69f1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc342e58a45445db864b6a66b797bfcb"
          }
        },
        "fe39a49fbb4a4dfaa459ebdf1b6ebed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6e30891266841f1afe8444d099ae450",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5000/5000 [10:58&lt;00:00,  7.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2cad1622c0cf42b2844609c5068873ff"
          }
        },
        "1587c0d296d441ee8eb5d293326f69f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc342e58a45445db864b6a66b797bfcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6e30891266841f1afe8444d099ae450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2cad1622c0cf42b2844609c5068873ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamdian0/Taxi-v3/blob/main/P1_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJkRJWp_iSxV"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn2DHS1HiSxb"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sYbduEAiSxc"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3961Sk5iSxc"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoF3qhqmiSxc"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mXEhFEZiSxd"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekHNMPHBiSxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef5e07a-58eb-49f9-9845-bff98da8b008"
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66FirzkTiSxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02e0840-efe6-4536-bd0b-fadf6deec738"
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Ys32SBiSxe"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec80Iv7DiSxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2da7d6-e11d-4679-c2c3-8a742fd58b69"
      },
      "source": [
        "taxi.reset()\n",
        "taxi.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNN0f9ZJiSxf"
      },
      "source": [
        "## Submission\n",
        "- Submit your solution as a Jupyter notebook. \n",
        "- Ensure that all cells in the notebook have been executed and the output is showing\n",
        "- Ensure that your solution consistently reaches the average cumulative reward defined in the rubric (link below)\n",
        "- Post your solution on Github and share the link to your commit as a direct message in Slack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3jIyb5siSxf"
      },
      "source": [
        "## Evaluation\n",
        "The goal of the project is to get an average (cumulative) reward of 9.0 or more over 100 episodes. To pass the project, you must meet all the requirments in the project [rubric](https://github.com/KnowchowHQ/rl-in-action/blob/master/C1-RL-Intro/W3OH/P1-rubric.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjKj9QqJNx1I"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Submission Starts From Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2_DU1sjlvNv"
      },
      "source": [
        "## DiscreteEnv Description\n",
        "Has the following members\n",
        "\n",
        "    - nS: number of states\n",
        "    - nA: number of actions\n",
        "    - P: transitions (*)\n",
        "    - isd: initial state distribution (**)\n",
        "    (*) dictionary of lists, where\n",
        "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "    (**) list or array of length nS\n",
        "\n",
        "Has following methods:\n",
        "\n",
        "    - __init__(): initialize the object\n",
        "    - seed(): initialize a random seed\n",
        "    - reset(): reset the state based on initial state distribution\n",
        "    - step(): take one step based on action, return a tuple of (state, reward, done flag, probability (probability of ending in the returned state by taking the specified action, i.e. slip probability is supported with step function, but in our case, they are all single action to single state mapping))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi8A3Gtel2AC"
      },
      "source": [
        "## Taxi V3 Description\n",
        "Has following members:\n",
        "\n",
        "    - desc: description of the MAP (static) used for Taxi-v3\n",
        "    - locs: location coordinates for RGBY markers (static)\n",
        "    - s: current state of the object\n",
        "    - lastaction: last action from step function\n",
        "\n",
        "  Has following methods:\n",
        "\n",
        "    - __init__(): initialize the object\n",
        "    - encode(): encode the taxi/passenger/destination locations into single number state representation\n",
        "    - decode(): decode the single number state representation into taxi/passenger/destination locations\n",
        "    - render(): print an ASCII map graph representation of current state\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piCpVfa5l8ep"
      },
      "source": [
        "## Taxi V3 Environment Description\n",
        "    The Taxi Problem\n",
        "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
        "    by Tom Dietterich\n",
        "    \n",
        "    Description:\n",
        "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "    \n",
        "    Observations:\n",
        "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
        "    Passenger locations:\n",
        "    - 0: R(ed)\n",
        "    - 1: G(reen)\n",
        "    - 2: Y(ellow)\n",
        "    - 3: B(lue)\n",
        "    - 4: in taxi\n",
        "    Destinations:\n",
        "    - 0: R(ed)\n",
        "    - 1: G(reen)\n",
        "    - 2: Y(ellow)\n",
        "    - 3: B(lue)\n",
        "    \n",
        "    Actions:\n",
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east\n",
        "    - 3: move west\n",
        "    - 4: pickup passenger\n",
        "    - 5: drop off passenger\n",
        "    \n",
        "    Rewards:\n",
        "    There is a default per-step reward of -1,\n",
        "    except for delivering the passenger, which is +20,\n",
        "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
        "    \n",
        "    Rendering:\n",
        "    - blue: passenger\n",
        "    - magenta: destination\n",
        "    - yellow: empty taxi\n",
        "    - green: full taxi\n",
        "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
        "    \n",
        "    state space is represented by:\n",
        "        (taxi_row, taxi_col, passenger_location, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_vhk0QNmDt4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Start of Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fcHA7nXOTZZ"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f2kjMiTNwXx"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from collections import deque\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIHpeJm8O2HG"
      },
      "source": [
        "## Helper Functions\n",
        "- pretty_print_policy(): customized pretty print function\n",
        "- epsilon_greedy_action_from_Q(): generate espilon greedy action\n",
        "- derive_terminal_states(): derive terminal states for easy use\n",
        "- greedy_policy_from_returns_tbl(): generate policy from return table\n",
        "- BFS_reward(): optimal BFS solution\n",
        "- single_test_compare(): single test with comparison to BFS optimal\n",
        "- n_episodes_test_compare(): test n episodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCNOB6qJO9sQ"
      },
      "source": [
        "### customized pretty print function ###\n",
        "def pretty_print_policy(policy, taxi=None, drop_off=False, render=True):\n",
        "    if taxi is None:\n",
        "      taxi = gym.make('Taxi-v3')\n",
        "    if render:\n",
        "      taxi.render()\n",
        "\n",
        "    taxi_row, taxi_col, pass_idx, dest_idx = taxi.decode(taxi.s)\n",
        "    if drop_off: pass_idx = 4\n",
        "\n",
        "    direction_repr = {1:' ü°ë ', 2:' ü°í ', 3:' ü°ê ', 0:' ü°ì ', None:' ‚¨§ ', 4:' O ', 5:' X '}\n",
        "\n",
        "    for row in range(5):\n",
        "        for col in range(5):\n",
        "            state = taxi.encode(row, col, pass_idx, dest_idx)\n",
        "            print(direction_repr[policy[state]],end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh3ox3wuPI6c"
      },
      "source": [
        "### generate espilon greedy action ###\n",
        "def epsilon_greedy_action_from_Q(Q, state, epsilon):\n",
        "    actions = Q.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "    \n",
        "    greedy_action_index = np.argmax(Q.loc[state].values)\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "\n",
        "    epsilon_greedy_action = np.random.choice(Q.columns,p=action_probs)\n",
        "    \n",
        "    return epsilon_greedy_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZUZ8AzjPaEM"
      },
      "source": [
        "### derive terminal states for easy use ###\n",
        "def derive_terminal_states(taxi):\n",
        "  terminal_states = []\n",
        "  for s in taxi.P:\n",
        "    for a in taxi.P[s]:\n",
        "      if taxi.P[s][a][0][3]:\n",
        "        terminal_states.append(taxi.P[s][a][0][1])\n",
        "  return terminal_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Img4OMFhPtDf"
      },
      "source": [
        "### generate policy from return table ###\n",
        "def greedy_policy_from_returns_tbl(table):\n",
        "    policy = {s:None for s in table.index }\n",
        "    for state in table.index:\n",
        "        if state not in terminal_states:\n",
        "            greedy_action = table.loc[state].idxmax()\n",
        "            policy[state] = greedy_action\n",
        "            \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQOCkPGmfcRV"
      },
      "source": [
        "### optimal BFS solution ###\n",
        "def BFS_reward(taxiloc, passloc, destloc):\n",
        "  M=N=5\n",
        "  ban_transitions = {((4,0),(4,1)),((4,1),(4,0)), ((3,0),(3,1)),((3,1),(3,0)), ((4,2),(4,3)),((4,3),(4,2)), ((3,2),(3,3)),((3,3),(3,2)), ((1,1),(1,2)),((1,2),(1,1)), ((0,1),(0,2)),((0,2),(0,1))}\n",
        "  n_step = 0\n",
        "\n",
        "  ti,tj = taxiloc\n",
        "  pi,pj = passloc\n",
        "  di,dj = destloc\n",
        "  q = deque()\n",
        "  visited = set()\n",
        "  q.append([ti,tj,0])\n",
        "  visited.add((ti,tj))\n",
        "  while q:\n",
        "    i,j,s = q.popleft()\n",
        "    if (i,j)==(pi,pj): \n",
        "      n_step += s\n",
        "      break\n",
        "    for ii,jj in [[i+1,j], [i-1,j], [i,j+1], [i,j-1]]:\n",
        "      if (0<=ii<M and 0<=jj<N and (ii,jj) not in visited and ((i,j),(ii,jj)) not in ban_transitions):\n",
        "        q.append([ii,jj,s+1])\n",
        "        visited.add((ii,jj))\n",
        "\n",
        "  q = deque()\n",
        "  visited = set()\n",
        "  q.append([pi,pj,0])\n",
        "  visited.add((pi,pj))\n",
        "  while q:\n",
        "    i,j,s = q.popleft()\n",
        "    if (i,j)==(di,dj): \n",
        "      n_step += s\n",
        "      break\n",
        "    for ii,jj in [[i+1,j], [i-1,j], [i,j+1], [i,j-1]]:\n",
        "      if (0<=ii<M and 0<=jj<N and (ii,jj) not in visited and ((i,j),(ii,jj)) not in ban_transitions):\n",
        "        q.append([ii,jj,s+1])\n",
        "        visited.add((ii,jj))\n",
        "\n",
        "  return 20-n_step-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uRiuC7gfoz6"
      },
      "source": [
        "### single test with comparison to BFS optimal, render=True will render a sequence of taxi movements ###\n",
        "def single_test_compare(policy, text=False, render=False, taxi_i=None, compare=False):\n",
        "  if taxi_i is None:\n",
        "    taxi_t = gym.make('Taxi-v3')\n",
        "    taxi_t.reset()\n",
        "  else:\n",
        "    taxi_t = copy.deepcopy(taxi_i)\n",
        "    \n",
        "  if render: taxi_t.render()\n",
        "\n",
        "  state = taxi_t.s\n",
        "  \n",
        "  if compare:\n",
        "    taxi_t0 = copy.deepcopy(taxi_t)\n",
        "    locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
        "    ti, tj, pass_idx, dest_idx = taxi_t.decode(state)\n",
        "    pi,pj = locs[pass_idx] if pass_idx<4 else (ti,tj)\n",
        "    di,dj = locs[dest_idx]\n",
        "    optimal_reward = BFS_reward((ti,tj), (pi,pj), (di,dj))\n",
        "    if text: print(f'BFS optimal reward = {optimal_reward}')\n",
        "\n",
        "  actions = {1:' ü°ë ', 2:' ü°í ', 3:' ü°ê ', 0:' ü°ì ', None:' ‚¨§ ', 4:' O ', 5:' X '}\n",
        "  done = False\n",
        "  max_step = 30\n",
        "  n_step = 0\n",
        "  reward_full = 0\n",
        "  bad = 0\n",
        "  if render: print('\\nActions based on policy table:')\n",
        "  while not done and n_step < max_step:\n",
        "    n_step += 1\n",
        "    \n",
        "    if text: print(f'action = {actions[policy_Q_Learning[state]]}')\n",
        "    state, reward, done, _ = taxi_t.step(policy_Q_Learning[state])\n",
        "    reward_full += reward\n",
        "\n",
        "    if render: taxi_t.render()\n",
        "  else:\n",
        "    if not done: bad = 1\n",
        "  \n",
        "  if text: print(f\"Full policy reward = {reward_full}\")\n",
        "\n",
        "  if compare and optimal_reward != reward_full:\n",
        "    return reward_full, bad, taxi_t0\n",
        "  else:\n",
        "    return reward_full, bad, None\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctODHqKhiZGP"
      },
      "source": [
        "### test n episodes ###\n",
        "### bad episodes refers to episodes that stuck in a possible loop beyond a maximum of 30 steps ###\n",
        "### non optimal taxi is a list of taxi instances that result in non optimal policies, and can be used for debugging and possibly a seperate dedicated training if needed, in practice I don't find this useful as the optimal policy is reached pretty quickly ###  \n",
        "def n_episodes_test_compare(policy, n=100, text=False, compare=False):\n",
        "  rewards = np.zeros(n)\n",
        "  total_bad = 0\n",
        "  non_optimal_taxi = []\n",
        "\n",
        "  for i in range(n):\n",
        "    reward, bad, t = single_test_compare(policy, compare=compare)\n",
        "    rewards[i] = reward\n",
        "    total_bad += bad\n",
        "\n",
        "    if t is not None:\n",
        "      non_optimal_taxi.append(t)\n",
        "\n",
        "  total_reward = sum(rewards)\n",
        "  if text: print(f'average reward = {total_reward/n}, bad episodes = {total_bad}, non optimal episodes = {len(non_optimal_taxi)}')\n",
        "  return total_reward/n, total_bad, non_optimal_taxi, rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m4ejOP7QGo9"
      },
      "source": [
        "## Main Q Learning Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566,
          "referenced_widgets": [
            "2550038c973a413694efb8a4cd109533",
            "f64013e08fe9496abb85069cd6d8017a",
            "0073afd7a7e6461f9ac81b57a9473a56",
            "fe39a49fbb4a4dfaa459ebdf1b6ebed2",
            "1587c0d296d441ee8eb5d293326f69f1",
            "dc342e58a45445db864b6a66b797bfcb",
            "d6e30891266841f1afe8444d099ae450",
            "2cad1622c0cf42b2844609c5068873ff"
          ]
        },
        "id": "HU6LnSB6QKdJ",
        "outputId": "9d4c53bb-16ff-43a8-f27d-5df323a38896"
      },
      "source": [
        "### Q-learning ###\n",
        "Q = pd.DataFrame.from_dict({s:{a:0 for a in range(taxi.action_space.n)} for s in range(taxi.observation_space.n)}, orient='index')\n",
        "terminal_states = derive_terminal_states(taxi)\n",
        "\n",
        "HYPER_PARAMS = {}\n",
        "HYPER_PARAMS['gamma'] = 1.0\n",
        "n_episodes = 5000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "### main turned hyper parameter ###\n",
        "alpha = 0.1\n",
        "\n",
        "non_optimal_taxi = []\n",
        "for i in tqdm(range(n_episodes)):\n",
        "  s0 = taxi.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "        a0 = epsilon_greedy_action_from_Q(Q,s0,epsilon)\n",
        "        s1, reward, done, _  = taxi.step(a0)\n",
        "        \n",
        "        Q.loc[s0,a0] += alpha*(reward + HYPER_PARAMS['gamma']*Q.loc[s1].max() - Q.loc[s0,a0])\n",
        "        s0 = s1\n",
        "  \n",
        "  epsilon *= epsilon_decay\n",
        "  epsilon = max(epsilon,min_epsilon)\n",
        "\n",
        "  ### monitoring 100 episode average reward for each 1000 training loop ###\n",
        "  ### bad episodes refers to episodes that stuck in a possible loop beyond a maximum of 30 steps ###\n",
        "  ### non optimal taxi is a list of taxi instances that result in non optimal policies, and can be used for debugging and possibly a seperate dedicated training if needed, in practice I don't find this useful as the optimal policy is reached pretty quickly ###  \n",
        "  if (i+1)%500==0:\n",
        "    print(f'\\nIteration {i+1}')\n",
        "    policy_Q_Learning = greedy_policy_from_returns_tbl(Q)\n",
        "    \n",
        "    ### n_episodes_test_compare with compare=True will test the episodes against an optimal BFS solution ###\n",
        "    ### in practice, this should not be done since \n",
        "    ###   1. we usually don't have a way to code optimal solution for many reinforced learning problems\n",
        "    ###   2. running optimal solution also increases the training time\n",
        "    avg_reward, n_bad_episodes, non_optimal_taxi, rewards = n_episodes_test_compare(policy_Q_Learning, text=True, n=100, compare=True)\n",
        "    \n",
        "policy_Q_Learning = greedy_policy_from_returns_tbl(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2550038c973a413694efb8a4cd109533",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 500\n",
            "average reward = -27.83, bad episodes = 95, non optimal episodes = 95\n",
            "\n",
            "Iteration 1000\n",
            "average reward = -14.41, bad episodes = 61, non optimal episodes = 61\n",
            "\n",
            "Iteration 1500\n",
            "average reward = -1.47, bad episodes = 26, non optimal episodes = 26\n",
            "\n",
            "Iteration 2000\n",
            "average reward = 1.61, bad episodes = 17, non optimal episodes = 17\n",
            "\n",
            "Iteration 2500\n",
            "average reward = 6.49, bad episodes = 5, non optimal episodes = 5\n",
            "\n",
            "Iteration 3000\n",
            "average reward = 7.96, bad episodes = 1, non optimal episodes = 1\n",
            "\n",
            "Iteration 3500\n",
            "average reward = 8.22, bad episodes = 0, non optimal episodes = 0\n",
            "\n",
            "Iteration 4000\n",
            "average reward = 7.68, bad episodes = 0, non optimal episodes = 0\n",
            "\n",
            "Iteration 4500\n",
            "average reward = 7.75, bad episodes = 0, non optimal episodes = 0\n",
            "\n",
            "Iteration 5000\n",
            "average reward = 7.45, bad episodes = 0, non optimal episodes = 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acbc2EPOkIC8"
      },
      "source": [
        "## Save/Load return table in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypAHbQENuXaG"
      },
      "source": [
        "# Do NOT run\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSOnpZUM11Ed"
      },
      "source": [
        "# Do NOT run\n",
        "#Q.to_pickle(f'drive/My Drive/Colab Notebooks/Q.{datetime.now()}.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfQqlVoiksg0"
      },
      "source": [
        "# Do NOT run\n",
        "#file_name = 'drive/My Drive/Colab Notebooks/Q.2021-04-28 23:35:03.032067.pkl'\n",
        "#Q = pd.read_pickle(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSK1JmjUlAlU"
      },
      "source": [
        "## Quick Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxNIoQS2DGYr"
      },
      "source": [
        "- print policy table for pick up based on a given taxi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwG6Gr5glm55",
        "outputId": "369c4ff6-26f5-4cf9-9bfc-bff89df3a632"
      },
      "source": [
        "taxi.reset()\n",
        "pretty_print_policy(policy_Q_Learning, taxi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            " ü°í  ü°ì  ü°ì  ü°ì  ü°ì \n",
            " ü°í  ü°ì  ü°ì  ü°ì  ü°ê \n",
            " ü°í  ü°í  ü°í  ü°ì  ü°ì \n",
            " ü°ë  ü°ë  ü°ë  ü°ì  ü°ê \n",
            " ü°ë  ü°í  ü°ë  O  ü°ê \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7q5YQZiDdnq"
      },
      "source": [
        "- print policy table for drop off (assuming passenger already picked up) based on a given taxi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQX3h_0ODl1A",
        "outputId": "ff2464bc-774d-4825-ad0e-4c3a12b5f80b"
      },
      "source": [
        "pretty_print_policy(policy_Q_Learning, taxi, drop_off=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            " ü°ì  ü°ê  ü°ì  ü°ì  ü°ê \n",
            " ü°ì  ü°ê  ü°ì  ü°ê  ü°ê \n",
            " ü°ì  ü°ê  ü°ê  ü°ê  ü°ê \n",
            " ü°ì  ü°ë  ü°ê  ü°ë  ü°ê \n",
            " X  ü°ë  ü°ë  ü°ë  ü°ê \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diCdwKUZDmpn"
      },
      "source": [
        "- below test shows a sequence of taxi actions based on trained policy table, we can visually verify the actions taken are optimal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh2yIlsbhlyC",
        "outputId": "c0b5a523-4b58-4548-9610-452f574641d0"
      },
      "source": [
        "single_test_compare(policy_Q_Learning, text=True, render=True, compare=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "BFS optimal reward = 9\n",
            "\n",
            "Actions based on policy table:\n",
            "action =  ü°ë \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "action =  ü°ê \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "action =  ü°ê \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "action =  ü°ê \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "action =  ü°ì \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "action =  ü°ì \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "action =  O \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "action =  ü°ë \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "action =  ü°ë \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "action =  ü°ë \n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "action =  ü°ë \n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "action =  X \n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Full policy reward = 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 0, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEgsp60clVqn"
      },
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSpnlRYKD7YO"
      },
      "source": [
        "- final evaluation is taken out of the training loop to avoid uncertainty caused by min epsilon\n",
        "- a curve is plotted for the running average reward of 100 episodes over 1000 episodes\n",
        "- 5th percentile and 95th percentile number is printed below the plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "clt5MJPclXtQ",
        "outputId": "bb9c1a42-145d-45cd-d7d0-e76db47cbe44"
      },
      "source": [
        "avg_reward, total_bad, non_optimal_taxi, rewards = n_episodes_test_compare(policy_Q_Learning, n=1000, text=True, compare=True)\n",
        "windowed_rewards = np.convolve(rewards, np.ones(100), 'valid')\n",
        "plt.plot(windowed_rewards/100)\n",
        "plt.show()\n",
        "print(f'5th percentile = {np.quantile(windowed_rewards/100, 0.05)}')\n",
        "print(f'95th percentile = {np.quantile(windowed_rewards/100, 0.95)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average reward = 7.881, bad episodes = 0, non optimal episodes = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5th percentile = 7.34\n",
            "95th percentile = 8.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqNIaG2Pln7I"
      },
      "source": [
        "## Project Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be2W6aOSFf9R"
      },
      "source": [
        "- Final training code is pretty simple, there is little change from the code we used in our class. However, there were lots of tuning, trials and experiments done before getting to the final cleaned up codes.\n",
        "\n",
        "- Such experiments include:\n",
        "  - playing around with different reward structure\n",
        "  - dynamically adjusting alpha values during training process\n",
        "  - customized pretty print function for policy table\n",
        "  - step by step visual analysis of taxi actions based on trained policy table\n",
        "  - tuning and adjusting of various hyper parameters\n",
        "  - developing optimal BFS solution to verify final training results\n",
        "  - etc.\n",
        "\n",
        "- Hyper parameter selection\n",
        "  - gamma: we picked the value of 1.0, since the original reward structure has -1 for default steps, there is no need for using a gamma value smaller than 1.0. However, based on experiments, using values such as 0.9 also converged the training with similar rate\n",
        "  - n_episodes: based on experiments, the training algorithm usually converges around 3000 iterations. One thing noticed is that since we did not apply a max length limit during training, the training speed is relatively slower at the beginning with around 6 it/s; when coming close to convergence, the training speed is much faster with around 80+ it/s\n",
        "  - alpha: this is found to be the most important hyper parameter that determines the convergence of the training algorithm for the project. The picked value is mostly based on trial and error. But some very rough estimation might be something like this: if alpha=0.001, then for the return table to reach the optimal value, it needs at least 1000 iterations; also at the beginning, since return table has all 0 values, it will take much more than 1000 iterations to reach the optimal value; however, we also observed that it is not necessary for the return table to reach optimal value to generate an optimal policy table, so there is still lots of uncertainty for this estimation. One general guideline I found that might be useful (could be wrong) is to use a relatively larger alpha to quickly find the trend of convergence, then adjust it to smaller values to improve training accuray or avoid over training, etc.\n",
        "  \n",
        "- The other thought is that for static environment, maybe we can train from backwards, since the final done state always have a fixed reward, so their neighouring cell will be able to derive optimal solution much easier. It is understood that in more realistic case, environment will be more dynamic and complex, in which case training from backwards may not be feasible or may have little difference compared to normal training."
      ]
    }
  ]
}